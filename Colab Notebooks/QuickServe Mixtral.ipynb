{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waAEIJBdDDtg"
      },
      "source": [
        "# MonsterAPI Deploy\n",
        "\n",
        "Introducing Monster Deploy. A new LLM Deployment engine that enables you to serve various LLMs along with lora adapters as an API endpoint on MonsterAPI's robust and cost optimised GPU Cloud.\n",
        "\n",
        "Following Deployment options are supported:\n",
        "1. Deploy SOTA LLMs and fine-tuned LLM LoRA adapters as a REST API serving endpoint\n",
        "2. Deploy docker containers for GPU powered applications\n",
        "\n",
        "Monster Deploy offers in-built optimisations for higher throughput and lower cost of serving LLMs.\n",
        "\n",
        "Checkout our [Developer Docs](https://developer.monsterapi.ai/docs/monster-deploy-beta)\n",
        "\n",
        "If you haven't applied for Deploy beta then you may signup on this [Google form](https://forms.gle/ZHuZt68fJLRozo3v9) for 10K credits. Sign Up with your organization ID to receive 30K credits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTJqHSqsD_3s"
      },
      "source": [
        "## Install MonsterAPI pypi client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slr71YbqDAUm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install monsterapi==1.0.2b3\n",
        "# Please install specific beta version of client for quick serve access."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qB0zjJLEzrR"
      },
      "source": [
        "Sign up on [MonsterAPI](https://monsterapi.ai/signup?utm_source=llm-deployment-colab&utm_medium=referral) and get a free auth key. Paste it below:\n",
        "Make sure you have signed up  for beta access at [here](https://forms.gle/TTJRapHm59RxjttJA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TNL3HjsEzYv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "api_key = \"YOUR_MONSTERAPI_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4cDy6EgEEuo"
      },
      "source": [
        "## Initialize client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWwbfgYKEHdT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from monsterapi import client as mclient\n",
        "deploy_client = mclient(api_key = api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpIG5bHtEPlq"
      },
      "source": [
        "## Launch a LLM deployment on MonsterAPI\n",
        "Let us deploy Mixtral 8x7b Chat model with GPTQ 4bit quantization by using a 48GB GPU.\n",
        "\n",
        "The Deployment will be able to serve the model as a REST API for both static and streaming token response support.\n",
        "\n",
        "An example of payload parameters for deploying Llama Model:\n",
        "\n",
        "```\n",
        "    basemodel_path=\"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\"\n",
        "    prompt_template=\"<s> [INST] {instruction} [/INST] {completion}</s>\"\n",
        "    api_auth_token=\"A_RANDOM_AUTH_TOKEN_TO_SECURE_YOUR_ENDPOINT\"\n",
        "    per_gpu_vram=48\n",
        "    gpu_count=1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLYZllWuEUar",
        "tags": []
      },
      "outputs": [],
      "source": [
        "launch_payload = {\n",
        "    \"basemodel_path\": \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ\",\n",
        "    \"prompt_template\": \"<s> [INST] {prompt} [/INST] {completion}</s>\",\n",
        "    \"api_auth_token\": \"b6a97d3b-35d0-4720-a44c-59ee33dbc25b\",\n",
        "    \"per_gpu_vram\": 48,\n",
        "    \"gpu_count\": 1,\n",
        "    \"use_nightly\": True\n",
        "}\n",
        "\n",
        "# Launch a deployment\n",
        "ret = deploy_client.deploy(\"llm\", launch_payload)\n",
        "deployment_id = ret.get(\"deployment_id\")\n",
        "print(deployment_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9gUNXReFPYv"
      },
      "source": [
        "### Deployment Status Types\n",
        "\n",
        "#### Below mentioned are different types of status responses:\n",
        "1. INPROGRESS:\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"pending\",\n",
        "        \"message\":\"Instance is still being provisioned, please wait and try again\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "2. Fail\n",
        "    ```\n",
        "    {\n",
        "        \"status\": \"failed\",\n",
        "        \"message\": \"Instance has failed, please launch a new instance\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "3. Building\n",
        "    ```\n",
        "    {\n",
        "        \"status\": \"building\",\n",
        "        \"message\": \"Server has started but trying to connect to deployment container, just downloading your model and setting things up, please try again in few mins, if state persists, please use /restart or /terminate!\"\n",
        "    }\n",
        "    ```\n",
        "4. Live\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"live\",\n",
        "        \"message\":\"Server has started !!!\",\n",
        "        \"URL\":\"https://c503a813-850a-4a78-93b9.monsterapi.ai\",\n",
        "        \"api_auth_token\":\"57b7b903-a4b6-4720-8154-af71aa8e8313\"\n",
        "    }\n",
        "    visit the url to get the llm service endpoint details or above url/docs to get swagger docs\n",
        "    ```\n",
        "5. Terminated by User\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"terminatedByUser\",\n",
        "        \"message\":\"Instance is terminatedByUser\"\n",
        "    }\n",
        "    ```\n",
        "\n",
        "6. Terminated by System (Out of  Credits)\n",
        "    ```\n",
        "    {\n",
        "        \"status\":\"terminatedBySystem\",\n",
        "        \"message\":\"Instance is terminatedBySystem\"\n",
        "    }\n",
        "    ```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-GadRntMRFL"
      },
      "source": [
        "## Get your Deployment Status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3n6CgwFO9os",
        "tags": []
      },
      "outputs": [],
      "source": [
        "status_debug = True # Just a placeholder to show possible statuses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkawseI4NxNq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "if status_debug:\n",
        "  status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "  print(status_ret)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SZqKMLSFWWv"
      },
      "source": [
        "#### Get Logs of deployment available from building status (This may take 5-10 minutes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFGFlRGDGEZx"
      },
      "source": [
        "> Deployment configuration may take few minutes. We are working on optimizing the service.\n",
        "\n",
        "> 'status' will be initially set to `building` and then to `live` as the deployment configuration progresses and the logs will be available from `building` state onwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-stvBsoFV0p",
        "tags": []
      },
      "outputs": [],
      "source": [
        "logs_ret = deploy_client.get_deployment_logs(deployment_id, n_lines = 50)\n",
        "if 'logs' not in logs_ret:\n",
        "  raise Exception(\"Please wait until status changes to building!\")\n",
        "for i in logs_ret['logs']:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooGyyGeYNJo0"
      },
      "source": [
        "#### Live Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9cvSEUEM-44",
        "tags": []
      },
      "outputs": [],
      "source": [
        "status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "print(status_ret)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9x1-SGvG986"
      },
      "source": [
        "## Once the deployment is live, let's query our deployed LLM endpoint:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo8e194fG7eA",
        "outputId": "3737702b-c1f0-4583-998e-6c49b85ec3d8",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/urllib3/connectionpool.py:999: InsecureRequestWarning: Unverified HTTPS request is being made to host 'c503a813-850a-4a78-93b9-e7a805916e66.monsterapi.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Why do you look so sad?\n",
            "\n",
            "I'm not sad, I'm just... contemplative.\n",
            "\n",
            "Contemplative? What does that even mean?\n",
            "\n",
            "It means I'm thinking deeply about life and all its mysteries.\n",
            "\n",
            "Mysteries? Like what?\n",
            "\n",
            "Like... like everything! The meaning of existence, the nature of reality, the purpose of humanity.\n",
            "\n",
            "But why do you have to think about all that stuff? Can't you just enjoy life and have fun?\n",
            "\n",
            "I do enjoy life, and I do have fun! But I also like to think about bigger questions. It's important to me to understand the world and my place in it.\n",
            "\n",
            "But don't you think that's a waste of time? You could be out there living life instead of sitting around thinking about it all the time.\n",
            "\n",
            "I don't think it's a waste of time at all. Thinking and living are not mutually exclusive. I can enjoy life and think deeply about it at the same time.\n",
            "\n",
            "But don't you get overwhelmed by all the big questions?\n",
            "\n",
            "Sometimes I do. But I try to remember that it's okay to not have all the answers. Life is a mystery, and that's what makes it so beautiful.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "assert status_ret.get(\"status\") == \"live\", \"Please wait until status is live!\"\n",
        "\n",
        "service_client  = mclient(api_key = status_ret.get(\"api_auth_token\"), base_url = status_ret.get(\"URL\"))\n",
        "\n",
        "payload = {\n",
        "    \"input_variables\": {\n",
        "        \"prompt\": \"What's up?\"},\n",
        "    \"stream\": False,\n",
        "    \"temperature\": 0.6,\n",
        "    \"max_tokens\": 2048\n",
        "}\n",
        "\n",
        "output = service_client.generate(model = \"deploy-llm\", data = payload)\n",
        "\n",
        "if payload.get(\"stream\"):\n",
        "    for i in output:\n",
        "        print(i[0])\n",
        "else:\n",
        "    print(json.loads(output)['text'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04II1U-EFoHQ"
      },
      "source": [
        "------\n",
        "\n",
        "## Terminate Deployment\n",
        "\n",
        "Once your work is done, you may terminate your LLM deployment and stop the account billing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkOyja-vFqU8",
        "outputId": "a56572f7-9638-40f2-8d33-e04401b333a5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'message': 'Instance Terminated'}\n"
          ]
        }
      ],
      "source": [
        "terminate_return = deploy_client.terminate_deployment(deployment_id)\n",
        "print(terminate_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN_aby3kROkz"
      },
      "source": [
        "## Terminate Status\n",
        "\n",
        "Get deployment status for confirmation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Li7xJuTRMjL",
        "outputId": "45cd8f68-b0e6-45d7-9e21-21ec947b3762",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'status': 'terminatedByUser', 'message': 'Instance is terminatedByUser'}\n"
          ]
        }
      ],
      "source": [
        "status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "print(status_ret)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}