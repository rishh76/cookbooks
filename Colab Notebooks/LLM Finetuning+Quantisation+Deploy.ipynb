{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_TLZdFdazVER"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Finetuning with MonsterTuner\n",
        "\n",
        "MonsterTuner is a no-code LLM finetuner for up to 10X more efficient and cost-effective finetuning of AI models for your business use-cases.\n",
        "\n",
        "### Supported Models for Finetuning:\n",
        "\n",
        "1. LLM (Large Language Model) - For use-cases like chat completion, summary generation, sentiment analysis, etc.\n",
        "2. Whisper - For speech-to-text transcription improvement.\n",
        "3. SDXL Dreambooth - Fine-tune Stable Diffusion model for customized image generation.\n",
        "\n",
        "\n",
        "Checkout our [Developer Docs](https://developer.monsterapi.ai/docs/launch-a-fine-tuning-job) on how to launch an LLM Finetuning Job with no-coding\n",
        "\n",
        "**How to finetune an LLM and Deploy it on MonsterAPI - [Complete Guide](https://blog.monsterapi.ai/how-to-fine-tune-a-large-language-model-llm-and-deploy-it-on-monsterapi/)**\n"
      ],
      "metadata": {
        "id": "IqENk2h8yZNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install monsterapi==1.0.8\n",
        "!pip install -q autoawq huggingface-hub peft"
      ],
      "metadata": {
        "id": "9JybrHe8y17T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sign up on [MonsterAPI](https://monsterapi.ai/signup?utm_source=llm-deploy-colab&utm_medium=referral) and get a free auth key. Paste it below:"
      ],
      "metadata": {
        "id": "ZK64gMDuy4KW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRowQKSnyC7-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from monsterapi import client as mclient\n",
        "import json\n",
        "import logging\n",
        "import tempfile\n",
        "from awq import AutoAWQForCausalLM\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import requests\n",
        "import os\n",
        "import zipfile\n",
        "from peft import PeftModel\n",
        "import huggingface_hub as hf_hub\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from huggingface_hub import HfApi, hf_hub_download, file_exists\n",
        "from accelerate import init_empty_weights\n",
        "\n",
        "os.environ['MONSTER_API_KEY'] = 'YOUR_MONSTER_API_KEY'\n",
        "client = mclient(api_key=os.environ.get(\"MONSTER_API_KEY\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch Finetuning Job\n",
        "\n",
        "This code block sets up `launch_payload` for fine-tuning an LLMs using specific configurations. The payload includes model path, LoRA parameters, data source details, and training settings such as learning rate and epochs. The model is fine-tuned using these settings"
      ],
      "metadata": {
        "id": "PxrorS9MKrCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "launch_payload = {\n",
        "    \"pretrainedmodel_config\": {\n",
        "        \"model_path\": \"facebook/opt-350m\",\n",
        "        \"use_lora\": True,\n",
        "        \"lora_r\": 8,\n",
        "        \"lora_alpha\": 16,\n",
        "        \"lora_dropout\": 0,\n",
        "        \"lora_bias\": \"none\",\n",
        "        \"use_quantization\": False,\n",
        "        \"use_gradient_checkpointing\": False,\n",
        "        \"parallelization\": \"nmp\"\n",
        "    },\n",
        "    \"data_config\": {\n",
        "        \"data_path\": \"tatsu-lab/alpaca\",\n",
        "        \"data_subset\": \"default\",\n",
        "        \"data_source_type\": \"hub_link\",\n",
        "        \"prompt_template\": \"Here is an example on how to use tatsu-lab/alpaca dataset ### Input: {instruction} ### Output: {output}\",\n",
        "        \"cutoff_len\": 512,\n",
        "        \"prevalidated\": False\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"early_stopping_patience\": 5,\n",
        "        \"num_train_epochs\": 1,\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"warmup_steps\": 50,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"lr_scheduler_type\": \"reduce_lr_on_plateau\",\n",
        "        \"group_by_length\": False\n",
        "    },\n",
        "    \"logging_config\": { \"use_wandb\": False }\n",
        "}\n",
        "\n",
        "\n",
        "ret = client.finetune(service=\"llm\", params=launch_payload)\n",
        "deployment_id = ret.get(\"deployment_id\")\n",
        "print(ret)"
      ],
      "metadata": {
        "id": "uaN93Po8y675"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch your Finetuning Job Status:\n",
        "\n",
        "Wait until the status is `Live`. It should take 5-10 minutes."
      ],
      "metadata": {
        "id": "4UEdXI-GzMgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get deployment status\n",
        "status_ret = client.get_deployment_status(deployment_id)\n",
        "print(status_ret)"
      ],
      "metadata": {
        "id": "NCIvdGfdzNCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "### Get Finetuning Job Logs\n",
        "\n",
        "To see your finetuning job progress, please run the cell below"
      ],
      "metadata": {
        "id": "k3qCzxOkzQ8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get deployment logs\n",
        "logs_ret = client.get_deployment_logs(deployment_id)\n",
        "print(logs_ret)"
      ],
      "metadata": {
        "id": "-oitFR1qzOhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "### Terminate Finetuning Job\n",
        "\n",
        "CAUTION: If you wish to terminate your finetuning job, please run the cell below"
      ],
      "metadata": {
        "id": "_TLZdFdazVER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Terminate Deployment\n",
        "# terminate_return = client.terminate_deployment(deployment_id)\n",
        "# print(terminate_return)"
      ],
      "metadata": {
        "id": "RDHWzEKkzTMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MonsterAPI LORA Merge and Quantization Notebook\n",
        "\n",
        "An accesory notebook to\n",
        "\n",
        "1.   Merge a lora adapter to its base model\n",
        "2.   Quantize it using the AWQ method\n",
        "3.   Push it to Huggingface repo.\n",
        "\n",
        "This notebook can directly accept a MonsterAPI model URL (ex:*https://finetuning-service.s3.us-east-2.amazonaws.com/finetune_outputs/cba26def-4cc6-476b-927a-6e1eff7d68e0/cba26def-4cc6-476b-927a-6e1eff7d68e0.zip*) or HuggingFace Repo Name (ex:*monsterapi/mistral_7b_DolphinCoder*) as input and can be used as an accessory after finetuning is complete in the main platform.\n",
        "\n"
      ],
      "metadata": {
        "id": "znC7QFd7M5dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = model_url = status_ret['info']['model_url']\n",
        "print(\"Model Path: \",model_path)\n",
        "# @title Model Configuration { display-mode: \"form\" }\n",
        "quantize = True #@param {type:\"boolean\"}\n",
        "hf_login_key = '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' #@param {type:\"string\"}\n",
        "hf_model_path = 'Deploy_Quantised_facebook_opt_350m' #@param {type:\"string\"}\n",
        "save_path = 'content/Final_Model' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Description of Parameters:\n",
        "#@markdown - `model_path`: MonsterAPI Finetuned model url or the HuggingFace repo name.\n",
        "#@markdown - `save_path`: Directory where the modified model should be saved after operations.\n",
        "#@markdown - `quantize`: Enable or disable model quantization. Set to `True` to apply quantization.\n",
        "#@markdown - `hf_login_key`: Authentication key for writing models hosted on Hugging Face. If not provided the model will not be pushed to huggingface\n",
        "#@markdown - `hf_model_path`: Repo name for saving it to huggingface\n"
      ],
      "metadata": {
        "id": "J4vrhKuaMyZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## LLM Quantization\n",
        "\n",
        "LLM (Large Language Model) Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\n",
        "\n",
        "Reducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.\n",
        "The Quantization we are going to be using is AWQ\n",
        "###AWQ Quantization\n",
        "Takes the concept of weight quantization to the next level by considering the activations of the model during the quantization process. In traditional weight quantization, the weights are quantized independently of the data they process. In AWQ, the quantization process takes into account the actual data distribution in the activations produced by the model during inference.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fLhMnmjkT0JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Management Utilities\n",
        "\n",
        "Lets write some utility functions designed for use later on. These include capabilities to download, unzip, quantize and save our model Functions covered:\n",
        "\n",
        "\n",
        "- `download_model_and_unzip`: Downloads and extracts model archives from specified URLs.\n",
        "- `merge_adapter`: Integrates adapter modules with base transformer models, optionally utilizing LoRA.\n",
        "- `quantize_and_load`: Applies quantization to models for efficient inference.\n",
        "- `save_model`: Saves the model and tokenizer to a specified directory for future use.\n"
      ],
      "metadata": {
        "id": "ZnhXSbr6TfoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)*0.8\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "EIMNfcJoBXCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe1280e-41ce-4414-9bca-99dddb892337",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 11.7984 GB.\n",
            "0.0 GB of memory reserved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimate Memory Requiements"
      ],
      "metadata": {
        "id": "u7mcG2lT_Rxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_params(model_name: str, trust_remote_code: bool = True, hf_token: str = None) -> int:\n",
        "    \"\"\"\n",
        "    Creates an empty model and calculates the number of parameters.\n",
        "    \"\"\"\n",
        "    with init_empty_weights():\n",
        "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=False, token=hf_token)\n",
        "        model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    return num_params\n",
        "\n",
        "from typing import Dict, List, Optional, Literal\n",
        "def estimate_memory_usage_hf(model_name, hf_token, lora_params_percentage) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Estimates the memory usage of the Hugging Face model.\n",
        "    \"\"\"\n",
        "\n",
        "    num_params = get_num_params(model_name, hf_token=hf_token)\n",
        "    model_size_gb = (num_params * 4) / (1024 ** 3)\n",
        "\n",
        "    # check if lora_config is provided and use it to calculate percentage params using num_params\n",
        "    # if lora_config:\n",
        "    #     lora_params_percentage = ((lora_config.l * lora_config.w * lora_config.r) / num_params) * 100\n",
        "\n",
        "    memory_usage = {}\n",
        "    dtype_sizes = {\n",
        "        'float32': 4,\n",
        "        'float16': 2,\n",
        "        'int8': 1,\n",
        "        'int4': 0.5\n",
        "    }\n",
        "\n",
        "    inference_scale_factor = 1.2\n",
        "    lora_scale_factor = (16 / 8) * 4 * inference_scale_factor\n",
        "\n",
        "    for dtype, size in dtype_sizes.items():\n",
        "        total_size = model_size_gb * (size / 4)\n",
        "        training_adam = total_size * 3.9\n",
        "        inference = total_size * inference_scale_factor\n",
        "        lora_trainable_params_gb = total_size * (lora_params_percentage / 100) * lora_scale_factor\n",
        "        lora_fine_tuning = total_size + lora_trainable_params_gb\n",
        "\n",
        "        memory_usage[dtype] = {\n",
        "            'inference': round(inference, 2),\n",
        "            'training_adam': round(training_adam, 2),\n",
        "            'lora_fine_tuning': round(lora_fine_tuning, 2)\n",
        "        }\n",
        "\n",
        "    return memory_usage\n",
        "\n",
        "def check_memory(max_memory, memory_usage):\n",
        "    # Directly extract memory usage values from the dictionary\n",
        "    memory_f16 = memory_usage['float16']['inference']\n",
        "    memory_f32 = memory_usage['float32']['inference']\n",
        "\n",
        "    if memory_f16 > max_memory:\n",
        "        print(\"Warning: Memory usage for float16 exceeds the limit. This colab notebook does not have enough memory for float16.\")\n",
        "    elif memory_f32 > max_memory:\n",
        "        print(\"Memory usage for float32 exceeds the limit.\", end=\" \")\n",
        "        if memory_f16 <= max_memory:\n",
        "            print(\"However, this notebook is suitable for float16 model.\")\n",
        "        else:\n",
        "            print(\"This colab notebook does not have enough memory for float16 either.\")\n",
        "    else:\n",
        "        print(\"This notebook is suitable for the given model using float32.\")"
      ],
      "metadata": {
        "id": "E1CQCzUcqOft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory Required to Execute"
      ],
      "metadata": {
        "id": "Syr4w7dx_YXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = launch_payload['pretrainedmodel_config']['model_path']\n",
        "memory_usage = estimate_memory_usage_hf(model_name=model_name,hf_token=hf_login_key,lora_params_percentage=1)\n",
        "print(f\"Memory Requirements for {model_name}: \",memory_usage)\n",
        "check_memory(max_memory, memory_usage)"
      ],
      "metadata": {
        "id": "8VCSnqVsrK9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Model Functions"
      ],
      "metadata": {
        "id": "4tmMSmli7kOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_model_and_unzip(url):\n",
        "\n",
        "    # Create a temporary directory\n",
        "    model_dir = tempfile.mkdtemp()\n",
        "\n",
        "    # Download the zip file\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    zip_path = os.path.join(model_dir, 'model.zip')\n",
        "    open(zip_path, 'wb').write(r.content)\n",
        "\n",
        "    if not os.path.exists(zip_path):\n",
        "        raise ValueError(f\"Failed to download model from {url}\")\n",
        "\n",
        "    # Unzip the zip file into the temporary directory\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(model_dir)\n",
        "\n",
        "    # Remove the zip file\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    # Verify model downloaded by checking if dir is empty\n",
        "    if len(os.listdir(model_dir)) == 0:\n",
        "        raise ValueError(f\"Failed to unzip model from {url}\")\n",
        "\n",
        "    # Return the temporary directory\n",
        "    return model_dir\n",
        "\n",
        "\n",
        "def merge_adapter(model_path):\n",
        "\n",
        "    #download\n",
        "    if model_path.startswith('http'):\n",
        "        model_path = download_model_and_unzip(model_path)\n",
        "    else:\n",
        "        hf_hub.snapshot_download(\n",
        "                            repo_id=model_path,  # type: ignore\n",
        "                            repo_type='model',\n",
        "                            local_dir=\"Final_Model\",\n",
        "                            local_dir_use_symlinks=False)\n",
        "        model_path = \"Final_Model\"\n",
        "\n",
        "\n",
        "    if os.path.isfile(model_path+'/adapter_config.json'):\n",
        "        with open(model_path+'/adapter_config.json', 'r') as f:\n",
        "            data = json.load(f)\n",
        "            basemodel_path = data['base_model_name_or_path']\n",
        "            loramodel_path = model_path\n",
        "            use_lora = True\n",
        "    else:\n",
        "        basemodel_path = model_path\n",
        "        use_lora = False\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(basemodel_path, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(basemodel_path,\n",
        "                                                 device_map='auto',\n",
        "                                                 trust_remote_code=True,\n",
        "                                                 low_cpu_mem_usage=True,\n",
        "                                                 torch_dtype=torch.bfloat16)\n",
        "    if use_lora==True:\n",
        "        logging.info(\"Loading lora model\")\n",
        "        model = PeftModel.from_pretrained(model, loramodel_path)\n",
        "        model = model.merge_and_unload()\n",
        "\n",
        "    return tokenizer,model\n",
        "\n",
        "\n",
        "def quantize_and_load(model_path):\n",
        "\n",
        "\n",
        "    quant_config = { \"zero_point\": True,\n",
        "                    \"q_group_size\": 128,\n",
        "                    \"w_bit\": 4,\n",
        "                    \"version\": \"GEMM\" }\n",
        "\n",
        "    model = AutoAWQForCausalLM.from_pretrained(\n",
        "        model_path, **{\"low_cpu_mem_usage\": True, \"use_cache\": False, \"device_map\": torch.device(\"cuda\")}\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "    model.quantize(tokenizer, quant_config=quant_config)\n",
        "\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "def save_model(tokenizer,model,save_path='Final_Model'):\n",
        "\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        os.system(f'rm -rf {save_path}')\n",
        "\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    return save_path"
      ],
      "metadata": {
        "id": "ZGQZRka-TAM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Execution\n",
        "\n",
        "Perform the final processing of the model, this will load the model, merge any lora adapters, quantize it and upload to huggingface as per the configuration set above."
      ],
      "metadata": {
        "id": "xZqZ2CIkTpNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(save_path):\n",
        "    os.system(f'rm -rf {save_path}')\n",
        "if model_path != '':\n",
        "    tokenizer, model = merge_adapter(model_path)\n",
        "    print('Successfully Merged and loaded the adapters')\n",
        "if save_path != '':\n",
        "    save_model(tokenizer,model,save_path)\n",
        "if quantize == True:\n",
        "    del tokenizer, model\n",
        "    tokenizer, model = quantize_and_load(save_path)\n",
        "    print('Successfully quantized and loaded the model')\n",
        "if save_path != '':\n",
        "    os.system(f'rm -rf {save_path}')\n",
        "    model.save_quantized(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    print(f'Model quantized and saved, it can be found in the ./{save_path} Directory')\n",
        "if hf_login_key != '':\n",
        "    try:\n",
        "        hf_hub.login(hf_login_key)\n",
        "        hf_model_path = hf_hub.whoami(token=hf_login_key)['name'] + '/' + hf_model_path.split('/')[-1]\n",
        "        tokenizer.push_to_hub(hf_model_path )\n",
        "        hf_api = hf_hub.HfApi()\n",
        "        hf_api.upload_folder(\n",
        "            folder_path=save_path,\n",
        "            repo_id=hf_model_path,\n",
        "            repo_type=\"model\"\n",
        "        )\n",
        "        logging.info('Model pushed to huggingface hub')\n",
        "    except Exception as e:\n",
        "        logging.warning('Failed to push to huggingface hub',e)"
      ],
      "metadata": {
        "id": "CGJKzBz7TiPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depoly Quantised Model as an API Endpoint"
      ],
      "metadata": {
        "id": "oLstE8hqaI9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = launch_payload['pretrainedmodel_config']['model_path']\n",
        "lora_model_path = status_ret['info']['model_url']\n",
        "\n",
        "launch_payload = {\n",
        "    \"basemodel_path\": base_model,\n",
        "    \"loramodel_path\": lora_model_path,\n",
        "    \"api_auth_token\": \"b6a97d3b-35d0-4720-a44c-59ee33dbc25b\",\n",
        "    \"prompt_template\": \"Here is an example on how to use tatsu-lab/alpaca dataset ### Input: {instruction} ### Output: {output}\",\n",
        "    \"per_gpu_vram\": 24,\n",
        "    \"gpu_count\": 1\n",
        "}\n",
        "\n",
        "# Launch a deployment\n",
        "ret = client.deploy(\"llm\", launch_payload)\n",
        "deployment_id = ret.get(\"deployment_id\")\n",
        "print(deployment_id)"
      ],
      "metadata": {
        "id": "CxkD8zwLWatO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Status of the Deployment"
      ],
      "metadata": {
        "id": "wwu6-gnSIfSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "status_ret = client.get_deployment_status(deployment_id)\n",
        "print(status_ret)\n",
        "assert status_ret.get(\"status\") == \"live\", \"Please wait until status is live!\"\n",
        "\n",
        "service_client  = mclient(api_key = status_ret.get(\"api_auth_token\"),base_url = status_ret.get(\"URL\"))"
      ],
      "metadata": {
        "id": "XFELBAluSb3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query the API Endpoint"
      ],
      "metadata": {
        "id": "szRH1Fy9rf0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "payload = {\n",
        "    \"input_variables\": {\n",
        "        \"instruction\": \"What is Global Warming?\"},\n",
        "    \"stream\": False,\n",
        "    \"temperature\": 0.6,\n",
        "    \"max_tokens\": 512\n",
        "}\n",
        "\n",
        "output = service_client.generate(model = \"deploy-llm\", data = payload)\n",
        "print(output['text'])"
      ],
      "metadata": {
        "id": "baMpiLuIUEfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "so4fWPObHQUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}