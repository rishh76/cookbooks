{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2bmt0-7QD9eX",
        "fn7yIDMgEPW5",
        "l2CdJlPyF0qD"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy `TinyLlama/TinyLlama-1.1B-Chat-v1.0` model on MonsterAPI using Monster Deploy\n",
        "\n",
        "Monster Deploy is a new LLM Deployment engine that enables you to serve various LLMs along with lora adapters as an API endpoint on MonsterAPI's robust and cost optimised GPU Cloud.\n",
        "\n",
        "Following Deployment options are supported:\n",
        "1. Deploy SOTA LLMs and fine-tuned LLM LoRA adapters as a REST API serving endpoint\n",
        "2. Deploy docker containers for GPU powered applications\n",
        "\n",
        "Monster Deploy offers in-built optimisations for higher throughput and lower cost of serving LLMs.\n",
        "\n",
        "Checkout our [Developer Docs](https://developer.monsterapi.ai/docs/monster-deploy-beta)\n",
        "\n",
        "If you haven't applied for Deploy beta then you may signup on this [Google form](https://forms.gle/ZHuZt68fJLRozo3v9) for early access with free credits."
      ],
      "metadata": {
        "id": "j1_OLTZMDVGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sign up on [MonsterAPI](https://monsterapi.ai/signup?utm_source=llm-deploy-colab&utm_medium=referral) and get a free auth key. Paste it below:\n",
        "Make sure you have signed up  for beta access at [here](https://forms.gle/TTJRapHm59RxjttJA)"
      ],
      "metadata": {
        "id": "1_X9ZhooDyK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGClBh86CpsZ"
      },
      "outputs": [],
      "source": [
        "api_key = \"YOUR_MONSTERAPI_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and Initialize MonsterAPI Client"
      ],
      "metadata": {
        "id": "2bmt0-7QD9eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install monsterapi==1.0.2b3\n",
        "\n",
        "from monsterapi import client as mclient\n",
        "deploy_client = mclient(api_key = api_key)"
      ],
      "metadata": {
        "id": "xLjNoU4aC1Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create `TinyLlama/TinyLlama-1.1B-Chat-v1.0` model deployment:"
      ],
      "metadata": {
        "id": "fn7yIDMgEPW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "{system} </s>\n",
        "<|user|>\n",
        "{prompt} </s>\n",
        "<|assistant|>\n",
        "{response}\n",
        "\"\"\"\n",
        "\n",
        "launch_payload = {\n",
        "    \"basemodel_path\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    \"prompt_template\": prompt_template,\n",
        "    \"per_gpu_vram\": 24,\n",
        "    \"gpu_count\": 1\n",
        "}\n",
        "\n",
        "# Launch a deployment\n",
        "ret = deploy_client.deploy(\"llm\", launch_payload)\n",
        "deployment_id = ret.get(\"deployment_id\")\n",
        "print(deployment_id)"
      ],
      "metadata": {
        "id": "vrish_dcEeNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch your Deployment Status:\n",
        "\n",
        "Wait until the status is `Live`. It should take 5-10 minutes."
      ],
      "metadata": {
        "id": "GlSQtAkcE8-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "status_ret = deploy_client.get_deployment_status(deployment_id)\n",
        "print(status_ret)"
      ],
      "metadata": {
        "id": "d81rpn_jE9bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Once the deployment is live, let's query our deployed LLM endpoint:"
      ],
      "metadata": {
        "id": "X8GVFhorFlJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "assert status_ret.get(\"status\") == \"live\", \"Please wait until status is live!\"\n",
        "\n",
        "service_client  = mclient(api_key = status_ret.get(\"api_auth_token\"), base_url = status_ret.get(\"URL\"))\n",
        "\n",
        "payload = {\n",
        "    \"input_variables\":  {\n",
        "                          \"system\": \"You are a friendly chatbot\",\n",
        "                          \"prompt\": \"Are you sentient?\"\n",
        "                        },\n",
        "    \"stream\": False,\n",
        "    \"temperature\": 0.9,\n",
        "    \"max_tokens\": 256\n",
        "}\n",
        "\n",
        "output = service_client.generate(model = \"deploy-llm\", data = payload)\n",
        "\n",
        "if payload.get(\"stream\"):\n",
        "    for i in output:\n",
        "        print(i[0])\n",
        "else:\n",
        "    print(json.loads(output)['text'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s8RwxAxFly8",
        "outputId": "d867947f-c052-4055-ff29-0cd4deac0f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host 'b8c8fd4b-b750-4c6e-a535-004cd872190c.monsterapi.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<|system|>\n",
            "You are a friendly chatbot </s>\n",
            "<|user|>\n",
            "Are you sentient? </s>\n",
            "<|assistant|>\n",
            "No, I am not sentient. Sentience is an idea that refers to the ability of living organisms to experience and have emotions, thoughts, and behaviors beyond those of a physical entity alone. Sentient refers to living biological entities that possess a form of intelligence and consciousness. While robots, artificial intelligence, and other machine-based entities may exhibit thought processes and behavior, they are not necessarily considered sentient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "\n",
        "### Terminate Deployment\n",
        "\n",
        "Once your work is done, you may terminate your LLM deployment and stop the account billing"
      ],
      "metadata": {
        "id": "l2CdJlPyF0qD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terminate_return = deploy_client.terminate_deployment(deployment_id)\n",
        "print(terminate_return)"
      ],
      "metadata": {
        "id": "10SbB8-CF2uu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}